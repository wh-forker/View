# PL-QA
+ Reference links:
	+ https://zhuanlan.zhihu.com/p/33124445
+ NarrativeQA	


## Dataset
+ SQuAD
	+ news
		+ 这个竞赛基于SQuAD问答数据集，考察两个指标：EM和F1。
		EM是指精确匹配，也就是模型给出的答案与标准答案一模一样；F1，是根据模型给出的答案和标准答案之间的重合度计算出来的，也就是结合了召回率和精确率。
		目前阿里、微软团队并列第一，其中EM得分微软（r-net+融合模型）更高，F1得分阿里（SLQA+融合模型）更高。但是他们在EM成绩上都击败了“人类表现”
	+ EMNLP2016 SQuAD:100,000+ Questions for Machine Comprehension of Text
		+ https://arxiv.org/pdf/1606.05250.pdf
	+ SQuAD，斯坦福在自然语言处理的野心
		+ http://blog.csdn.net/jdbc/article/details/52514050
	+ 一共有107,785问题，以及配套的 536 篇文章
	+ 数据集的具体构建如下：
		1. 文章是随机sample的wiki百科，一共有536篇wiki被选中。而每篇wiki，会被切成段落，最终生成了23215个自然段。之后就对这23215个自然段进行阅读理解，或者说自动问答。
		2. 之后斯坦福，利用众包的方式，进行了给定文章，提问题并给答案的人工标注。他们将这两万多个段落给不同人，要求对每个段落提五个问题。
		3. 让另一些人对提的这个问题用文中最短的片段给予答案，如果不会或者答案没有在文章中出现可以不给。之后经过他们的验证，人们所提的问题在问题类型分布上足够多样，并且有很多需要推理的问题，也就意味着这个集合十分有难度。如下图所示，作者列出了该数据集答案的类别分布，我们可以看到 日期，人名，地点，数字等都被囊括，且比例相当。
		4. 这个数据集的评测标准有两个：
			第一：F1
            第二：EM。
            EM是完全匹配的缩写，必须机器给出的和人给出的一样才算正确。哪怕有一个字母不一样，也会算错。而F1是将答案的短语切成词，和人的答案一起算recall，Precision和F1，即如果你match了一些词但不全对，仍然算分。
		5. 为了这个数据集，他们还做了一个baseline，是通过提特征，用LR算法将特征组合，最终达到了40.4的em和51的f1。而现在IBM和新加坡管理大学利用深度学习模型，均突破了这个算法。可以想见，在不远的将来会有更多人对阅读理解发起挑战，自然语言的英雄也必将诞生。甚至会有算法超过人的准确度。

+ 对比
	```
    当前的公开数据集对比如下，MCTest，Algebra和Science是现在的三个公开的阅读理解数据集，我们可以看到Squad在数量上远远超过这三个数据集，这使得在这个数据集上训练大规模复杂算法成为可能。同时，相比于WikiQA和TrecQA这两个著名问答数据集，Squad也在数量上远远超过。而CNN Mail和CBT虽然大，但是这两个数据集都是挖空猜词的数据集，并不是真正意义上的问答。
    ```
    
