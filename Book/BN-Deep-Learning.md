+ #(15) represent learning
 + Share statistical information in diff models 
 + 分布式表示学习
 	+ 聚类
 	+ K-近邻
 	+ 决策树
 	+ 高斯混合体和专家混合体
 	+ 具有高斯核的核机器
 	+ 基于n-gram的语言或翻译模型


+ #(16) structured probalilistic model
+ ##非结构化建模的挑战
	+ 基于表格操作计算量太大


+ ## 使用图描述模型结构
	+ 有向模型
		+ 也称为信念网络（belief network）或者贝叶斯网络（Bayesian network）
		+ 有箭头指向的:a 指向 b, 说明b的概率分布依赖a的取值
		+ 正式: x的有向概率模型通过有向无环图和局部条件概率分布来定义
				$$p(t_0,t_1,t_2) = p(t_0)p(t_1|t_0)p(t_2|t_1)
                $$
	+ 无向模型
        + 也称马尔可夫随机场(Markov random field)，或马尔可夫网络(Markov network)
        + 相互作用间没有明确的方向
        + 正式：无向模型是一个定义在无向图(原文是无向模型)上的结构化概率模型，对于图中的每一个团 *C*，一个因子$\phi(C)$,(也称团势能)，衡量了团中变量每一种可能的联合状态所对应的密切程度。这些因子都被限制为非负的，他们一起定义了 未归一话概率（unnormalized probability function）:
        		$$
                	\widetilde{p}(x) = {\prod_{c\epsilon G}^{}}\phi (C)
                $$
		+ 定义: 图中的团是指图中节点的一个子集，并且其中的节点是全连接的
		+ 只要所有团中的节点数不大，我们就可以高效处理这些未归一化概率（？？？）
		+ 思想 : 密切度越高的状态有越大的概率
	+ 配分函数
        + 背景: 归一化概率分布函数
        		$$
                	p(x) = \frac{1}{Z}\widetilde{p}(x)
                $$
                其中，Z是使所有概率之和或积分为1的常数，并且满足：
                $$
					Z = \int \widetilde{p}(x)  d(x)
                $$
        + Z通常认为是常数，且很难求解，一般使用近似方法，
        + 吉布斯分布(Gibbs distribution)一个通过归一化团势能乘积定义的函数
    	+ 设定一些使得Z不存在的因子也是有可能的（？？？）
	+ 基于能量的模型(Energy based model, EBM)
        + 无向模型大多依赖这个假设：  $\forall(x), \widetilde{p}(x)>0$
        + 这种假设可通过EBM实现，其中:
        	 $$
                	\widetilde{p}(x)=exp(-E(x))
             $$
             E(x) 被称为能量函数， 对于所有z, E(z)都是正的
             负号是为了保证机器学习文献和物理学算法之间的兼容性
             保证了没有一个能量函数会使得某一个状态的概率为0
             可以选择那些能够简化学习过程的能量函数
             + 直接学习各个团的势能时，需要利用约束优化方法来指定一下特定的概率最小值
             + 能量学习是，可以采用无约束的方法(EBM中概率可以无限接近0,但不是0)
      	+ 服从上式分布的称为波尔兹曼分布(Boltzmann distribution)的一个实例，基于能量的模型成为波尔兹曼机
             + 拥有浅变量的模型
             + 没有浅变量的波尔兹曼机通常被成为马尔可夫随机场或对数线性模型
		+ 无向模型中的不同的团对应能量函数的不同项，即基于能量的模型是一种特殊的马尔可夫模型
		+ 如何从无向模型中获得能量的函数形式(+++)
		+ 对概率模型进行操作的算法不计算$p_{model}(x)$，而是计算$log\widetilde{p}_{model}(x)$

	+ 分离和d-分离
        + 背景：确定变量子集间的条件独立
        + 分离：
        	+ 无向模型识别条件对立较为简单，图中隐含的条件独立性称为分离(separation)
        	+ 定义：
        	+ 两个例子需要添加(+++)
        + d-分离:
        	+ 定义：与分离一样
        	+ 两个变量间有活跃路径，则称为是依赖的，如果没有活跃路径，则称为d-分离
        	+ 例子：如何判断d-分离(???)
	+ 在有向图和无向图之间转换
       	+ 受限波尔兹曼模型称为无向模型，稀疏编码称为有向模型
       	+ 如何选择取决于要描述的概率分布
       		+ 哪种方法可以最大程度的捕捉到概率分布中的独立性
       		+ 哪种方法可以使用最少的边来描述概率分布
       		+ 其他因素
       		+ 即使使用单个概率分布，也可以在不同的建模方法间切换
       		+ 如果观察到变量的某个子集，或者希望执行不同的计算任务，换一种建模方法可能会更好
       		+ 例如：
       			+ 有向模型通常提供从模型中高效抽取样本的直接方法
        		+ 无向模型对于推导近似推断过程较为有用
        	+ 完全图
        		+ 图模型的优势：包含一些变量不直接作用的信息
        		+ 完全图缺点：不隐含任何独立性
        	+ 有向模型和无向模型各自的表示能力
        		+ 1(+++)
        		+ 2(+++)
	+ 因子图
    	+ factor graph : 从**无向模型**中抽样的另一种方法
        + 可以解决**标准**无向模型语法中**图表达**的模糊性
        + 模糊性：（+++）
        + 通过显式的表示每一个$\phi$函数的作用域，因子图解决了这种模糊性
        + 列子(+++)


+ ## 从图模型中采样
    + 图模型同样简化了从模型中采样的过程
    + **有向图模型**的一个优点是，通过一个简单高效的过程从模型所表示的联合分布中产生样本，这个过程被称为原始采样(ancestral sampling)
    + 原始采样的**基本思想**：将图中的变量 $ x$ 使用拓扑排序，使得对于所有i和j,如果$x_{i}$是$x_{j}$的父节点，则j大于i。然后可以按照此顺序进行采样
    	+ 特点：(+++)
    	+ 缺点：仅适用于有向模型，无向模型要转化为有向模型，转化后还有新的问题
   		+ 从无向模型采样，无法确定明确的起点
   			+ 需要较高的迭代次数
   			+ 理论上较简单的方法是Gibbs 采样(Gibbs Sampling)
   		+ 无向模型的采样技术是一个较为高级的研究方向，（17）蒙特卡罗方法会有较为详细的介绍   


+ ## 结构化建模的优势
    + 优势：可以显著的降低表示概率的分别、学习和推断的成本
    + **有向模型**的采样还可以被加速，但对于**无向模型**情况较为复杂，选择不对某些变量的相互作用来建模是减少运行时间和内存的主要机制
    + 结构化概率模型允许将**现有知识**与知识的学习和推断分开
    	+ 模型的开发和调试会更加容易
    	+ 可以设计、分析和评估适用于**更广范围**的图的学习算法和推断算法，设计可以捕获数据中重要关系的模型;之后，可以组合不同的算法和结构，并获得不同可能性的笛卡尔积
  		+ 缺点：设计端对端模型较为困难


+ ## 学习依赖关系
    + 建模依赖关系的方法通过引入‘隐藏’来操作
    	+ 变量$v_{i}$和变量$v_{j}$之间的间接依赖关系可以通过 $v_{i}$ 和 h 之间的直接依赖与 $v_{j}$ 和 h 之间直接依赖来建模
    	+ 如果关于 v 的模型不包含任何浅变量，则
    		+ 贝叶斯网络中需要有大量的父节点
    		+ 马尔可夫网络中具有非常大的团
    	+ 结构学习
    	+ 浅变量优势
    		+ 避免离散搜索和多轮训练
    		+ 新变量 h 提供 v 的替代表示


+ ## 推断和近似推断
   + 观察变量 v, 其特征为$E[h | v]$
   + 用极大似然估计来训练模型
   + 由于
   		$$
        	log p(v) = E_{h～p(h|v)}[log p(h,v)-log p(h|v)]
        $$
        中要计算p(h|v)，属于推断问题，在给定一些变量的情况下推断下一些变量的值
   + 在深度模型中很难处理推断问题
       + P-Hard NP (???)
       + 近似推断，涉及变分推断


+ ## 结构化概率模型的深度学习方法
    + DL 中不经常涉及较深的图模型
    + 图模型中根据图模型的深度而不是计算图来定义模型深度
    + 深度的定义：浅变量到观察变量的最短路径，不同于计算图的深度
    + 用于深度学习的许多生成模型没有浅变量或只有一层浅变量，但使用深度计算图来定义模型中的条件分布
    + 深度学习利用分布式表示的思想
    	+ （+++）
    	+ 模型中有比可观察变量更多的浅变量，变量之间的复杂非先行作用通过多个浅变量的间接连接来实现
    + 图模型中包含至少是偶尔观察到的变量，即使一些训练样本中的许多变量随机的丢失
    	+ 传统模型大多使用高阶项和结构学习来捕获变量间复杂的非线性关系
    	+ 如果有浅变量，数量通常很少
    + 浅变量设计方式在DL中也有所不同
    	+ 希望浅变量事先包含特定的含义--其含义又训练算法来开发
    + 连接类型在DL中也经常被使用
    	+ 深度图模型：通常具有大的与其他单元组全连接的单元组，使得两个组之间的相互作用可以用单个矩阵来表示
    	+ 传统图模型：
    		+ 具有非常少的连接，每个变量的连接选择可以单独设计
    		+ 模型的结构设计与推断算法的选择紧密相关
    		+ 旨在保持精确推断的可解释性
    		+ 当约束太强时：可采用环状信念传播(loopy belief propagation)的近似推断算法
    	+ 区别
    		+ 深度图模型从不使用环状信念传播
    		+ 深度学习模型可用来进行加速Gibbs采样或者变分推断
    		+ 深度学习包含大量浅变量，需要高效的数值计算代码
    		+ 深度图模型提高了对未知量的容忍度
    			+ 不进行模型简化（让每一个量可以被计算），仅仅使用数据进行运算和训练，以增强模型的能力

    ### 受限波尔兹曼机
		+ 图模型应用与DL的典型例子
		+ RBM 是浅层模型
		+ 单元被分做组，这种组被称作层，层间用矩阵来描述，联通性相对密集
		+ 可以高校的进行Gibbs采样
		+ 可以以很高的自由度来学习浅变量，浅变量的含义不是设计者指定的
		+ 标准的RBM是具有二值的**可见单元**和**隐藏单元**的基于能量的模型，其能量函数为：
		$$
        	E(v,h) = -b^Tv-c^Th-v^TWh
        $$
        	+ b,c,W 都是无约束、实值的可学习参数
        	+ 模型被分为v,h两组参数，其相互作用用W来描述
        	+ 同层不相连，不同层全连接
       	+ 对RBM 结构的限制产生了良好的属性
       	$$
        	p(h|v) = \prod_{i}p(h_{i}|v)
        $$
        以及
        $$
        	p(v|h) = \prod_{i}p(v_{i}|h)
        $$
        		1. 独立同分布的条件很容易计算（高效的Gibbs计算）
        		2. 能量函数本身只是参数的线性函数，很容易获得能量函数的导数
        + RBM展示了图模型的深度学校方法：
        	+ 使用多层变量
        	+ 由参数矩阵表示层之间的相互作用



+ #(17)蒙特卡罗方法
	+ 随机算法可以分为两类：Las Vegas算法 和 蒙特卡罗算法
	+ Las Vegas 算法总是返回一个正确答案(或者返回算法失败了)，次方法会占用随机量的计算资源(内存和计算资源)
	+ 蒙特卡罗算法返回的答案具有随机大小的错误，花费更多的计算资源可以减少这种错误，在固定的计算资源下，蒙特卡罗方法会得到近似解


+ ## 采样和蒙特卡罗方法
	+ 采样
		+ 以较小的代价近似许多项的和或积分
			+ 加速很费时却易于处理
			+ 难以处理的求和或积分
				+ 估计无向模型中函数对数的梯度
	+ 蒙特卡罗采样基础
		+ **蒙特卡罗方法思想**：把和或者积分当作某分布下的**期望**
			+ 令
			$$
        		s = \sum_{x}p(x)f(x) = E_{p}[f(x)]
        	$$
        	或者
        	$$
        		s = \int p(x)f(x)dx = E_{p}[f(x)]
        	$$
        + **通过采样计算期望**：从p中抽取n个样本$x^{(1)},...,x^{(n)}$ 来近似s并得到一个经验平均值
        	$$
            	\hat{w}_n = \frac{1}{n} \sum^{n}_{i=1}f(x^{i})
            $$
        + 几个性质证明这种近似是合理的
        	+ 这种估计是无偏的(估计量的数学期望等于被估计参数的真实值)
        	+ 根据大数定理(Law of large number):如果样本是独立同分布的，其平均值几乎必然收敛于期望值(估计值等于理论值)
        	$$
            	lim_{n \to \infty} \hat{s}_n= s
            $$
        	+ 单项方差有界(+++)
        	+ 中心极限定理（theory of limit theorem）: $\hat{s}_n$ 的分布收敛到以s为均值以$\frac{[f(x)]}{n}$为方差的正态分布，这使得可以利用正态分布的累积函数来估计置信区间
	+ 以上结论依赖于可以从基准分布中p(x)中轻易采样，当无法采样时需要用 **重要采样**

+ ## 重要采样
+ ## 马尔可夫蒙特卡罗方法
+ ## Gibbs采样
+ ## 不同峰值之间的混合

# 配分函数
## 对数似然梯度
## 随机最大似然和对比梯度
## 伪似然
## 得分匹配和比率匹配
## 去燥得分匹配
## 噪声对比估计
## 估计配分函数

#近似推断